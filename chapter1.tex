\chapter{INTRODUCTION}
\label{chap:introduction}
The scientific process increasingly benefits from the use of computation to achieve advances faster. Many times these computations can be naturally broken into steps, where each step may filter, transform or compute on the data it receives as input from a previous step.  It has been widely adopted to model and express these computations as a directed acyclic graph (DAG) of computations, or a workflow (see \cite{liu2015survey}).  These workflows (or scientific workflows) have many tasks that take a long time to compute and that produce a considerable amount of intermediate datasets.  Because of the nature of scientific exploration, a scientific workflow is usually modified and re-run multiple times, or new scientific workflows are created that might make use of past intermediate datasets.  

Storing intermediate datasets has the potential to save time in computations. Since storage is limited, one main problem that needs a solution is determining which intermediate datasets need to be saved in order to minimize the computational time of workflows that will be submitted to the system in the future. Some systems provide a solution to the problem (\cite{yuan2012data}), and there have been efforts in the research of algorithms that choose to save the right intermediate datasets in order to optimize the computation time of the workflows to be computed (see \cite{bazzi}). But the research effort has been mainly directed to algorithms that can make optimal decisions with full knowledge of the future workflow submissions.  In realistic scenarios this might not be possible, since due to the nature of scientific exploration, researchers usually need to use the results of current computations in order to design future workflows.

Another current issue in the field is the divide that exists between academic and industry-level workflow systems.  Academic systems are mostly responsible for the use directed acyclic graphs as the model to express workflows of computations.  They have also been adopting the latest research in the area of data reuse optimization algorithms.  On the other hand, industry-level workflow systems such as Apache Oozie and Apache Airflow have mainly focused in managing the execution of workflows of highly scalable computations that run in the successful Hadoop ecosystem. Both Oozie and Airflow have great monitoring capabilities and are very extensible. Unfortunately, they don't provide any data reuse functionality.  There is the need for a system that can manage highly scalable computations and that provides support for intermediate computations and data reuse.

In order to tackle those two issues, I have created Pingo, a system that is capable of managing the computations of scientific workflows for the Hadoop ecosystem. Pingo also provides a solution to the problem of optimization of data reuse without knowledge of future submissions of workflows, since it is capable, under some general and reasonable assumptions, of predicting what intermediate datasets will be needed by future workflow submissions to the system. Pingo also takes care of the management of storage and provenance information of the intermediate datasets. This work describes the design and implementation of the system's features.  The need for most of those features is established from research as well as from my experience using scientific workflow systems in the past. 

\section{Contributions}
Pingo has a commonality with previous systems \citep{altintas2004kepler, yuan2012data, deelman2015pegasus} in that it stores intermediate datasets produced by workflows with the purpose of skipping the computations of future workflows. But differing from previous systems, Pingo does it in a "reactive way".  That is, most previous systems decide which intermediate datasets to keep in storage with foreknowledge of the definition of the future workflows that will be submitted to the system.  In this work, I analyze the history of workflows already submitted to the system in order to determine which datasets might be important to keep for the future. That difference in design makes it suitable for fast-paced research environments where it is impossible for the researcher to foresee what are the next steps to take.

Another important contribution of this work is that, to the best of my knowledge, this is the first system bringing management of intermediate datasets and its computational optimization advantages to the Hadoop ecosystem.  A consequential contribution of designing it with Hadoop in mind is that we have also designed it to be an scalable multi-user system.  There are other smaller contributions scattered throughout the thesis. For example, in order to support data reuse, the system needs to make use of a data provenance system that keeps track of the origin of each dataset stored in the file system. There are many ways of implementing such system, and I propose a hashing mechanism as an efficient alternative to determining the provenance of datasets.  

The rest of the thesis is organized as follows: Chapter \ref{chap:related} presents research work related to the data reuse problem and describes existing systems for managing scientific workflows.  Chapter \ref{chap:foundational} formally introduces the problem and explores the theoretical motivations behind the design decisions and tradeoffs of the system. It also presents two families of algorithms to solve the problem. Chapter \ref{chap:implementation} provides a detailed description of the design and implementation of the system.  In Chapter \ref{chap:evaluation} proposes a methodology to evaluate the performance of the decision algorithms proposed in \ref{chap:foundational}.  It also reports the results of that evaluation methodology on Pingo.  Finally, Chapter \ref{chap:future} talks about the new possibilities of research that can improve and add to the functionality of the system.