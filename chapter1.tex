\chapter{INTRODUCTION}
\label{chap:introduction}
The scientific process increasingly benefits from the use of computation to achieve advances faster. Many times these computations can be naturally broken into steps, where each step may filter, transform or compute on the data it receives as input from a previous step.  Workflows have emerged as a paradigm for representing these computations. The large 




The scale of computations have been growing with time, and the ability of the systems cited above to process large amounts of data and to execute the placement of task execution on a distributed environment is still very limited.  

One important capability that has been added as a functionality to some of these systems (see \cite{yuan2012data}) is the ability to store the computations of intermediate datasets with the purpose of optimizing the computation time of future workflows that are to be computed by the system and that make use of those intermediate datasets. For a scientific workflow system, storing all the intermediate data generated during workflow executions may cause high storage cost.  On the contrary, if we delete all the intermediate data and regenerate them every time when ever needed, the computation cost of the system may also be very high. Because of that, good tradeoffs need to be found to solve the problem.  They are sometimes achieved by constraining the amount of storage space available to the system, and other times by assigning cost to both storage space and computation time and designing decision algorithms that optimize for that cost. 

Orthogonal to the development of workflow management systems, many distributed systems have been designed and developed to meet the growing demands of computation.  One of such systems is Apache Hadoop, which is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Apache Oozie\cite{islam2012oozie} is Apache Hadoop's workflow and scheduling system.  Its workflow definition API rivals that of Pegasus, while it takes advantage of the superior scalability of Hadoop. Unfortunately, it does not provide any capability to optimize the computation of workflows by saving the output of intermediate datasets in the hope of skipping the computation of future actions submitted to the system.


\section{Related Work}
Many seminal works on the topic of workflow managing systems began to appear in the mid 2000's \citep[e.g.]{yu2005taxonomy, fox2006special, gil2007examining}, and many workflow systems were developed, such as the e-Science project\citep{deelman2009workflows}, Kepler\cite{altintas2004kepler} and Taverna\cite{oinn2006taverna}.  

Pegasus \citep{singh2008workflow} is a more recent workflow management system for scientific applications.  It enables workflows to be executed both locally and on a cluster of computers in a simultaneous manner.  It has a rich set of APIs that allow the construction and representation of workflows as Directed Acyclic Graphs (DAGs).  It also has more advanced job scheduling and monitoring facilities than previous systems.  

\section{Our Contributions}
The main contribution of our work is the design and implementation of a system for the management of computations and storage of scientific workflows with constrained storage space. Our system has a commonality with previous systems in that it stores intermediate datasets produced by workflows with the purpose of skipping the computations of future workflows.  In order to decide which datasets should be kept or not, we have opted for the  approach of constraining the storage space available to the system, keeping datasets that will optimize the computational time of workflows under such constraint. But differing from previous systems, our system does it in a "reactive way".  That is, most other systems decide which intermediate datasets to keep on storage with foreknowledge of the definition of the future workflows that will be submitted to the system.  In our case, we analyze the history of workflows already submitted to the system in order to determine which datasets might be important to keep for the future. That difference in design makes it suitable for fast-paced research environments where it is impossible for the researcher to foresee what are the next steps to take.

Another important contribution of our work is that, to the best of our knowledge, this is the first system bringing those computational optimization capabilities to the Hadoop ecosystem.  A consequential contribution of designing it with Hadoop in mind is that we have also designed it to be an scalable multi-user system.

There are other smaller contributions scattered throughout the report. For example, we propose an encryption mechanism as an efficient alternative to determine the provenance of datasets produced by actions.  Chapter \ref{chap:foundational} of our discussion is also a good contribution to the topic of scientific workflows, since it contains a throughout explanation of the tradeoffs and balancing acts that need to be addressed (or at least considered) when designing a system that promises a functionality similar to the one we are attempting in our project. 


