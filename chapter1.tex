\chapter{INTRODUCTION}
\label{chap:introduction}
The scientific process increasingly benefits from the use of computation to achieve advances faster. Many times these computations can be naturally broken into steps, where each step may filter, transform or compute on the data it receives as input from a previous step.  It has been widely adopted to model and express these computations as a directed acyclic graph (DAG) of computations, or a workflow (see \
cite{liu2015survey}).  These scientific workflows have many tasks that take a long time to compute, and produce a considerable amount of intermediate datasets.  Because of the nature of scientific exploration, a scientific workflow is usually modified and re-run multiple times, or new scientific workflows are created that might make use of the intermediate datasets.  Storing intermediate datasets has the potential to save time in computations. Since storage is limited , one main problem that needs a solution is determining which intermediate datasets need to be saved at creation time in order to minimize the computational time of the workflows to be run in the future.

There are a number of publications that have focused on particular aspects and versions of the data reuse problem.  In this research I provide an integrated view of Pingo, a system that I have designed, capable of managing the computations of scientific workflows. Our work describes system features that are derived from research as well as from experience using scientific workflow systems in the past. Pingo provides a solution to the problem of data reuse of intermediate datasets since it is capable, under some general and reasonable assumptions, of predicting what intermediate datasets will be needed by future workflow submissions to the system. Pingo also takes care of the management of storage and provenance information of the intermediate datasets.

\section{Related Work}
Many seminal works on the topic of workflow managing systems began to appear in the mid 2000's \citep[e.g.]{yu2005taxonomy, fox2006special, gil2007examining}, and many workflow systems were developed, such as the e-Science project\citep{deelman2009workflows}, Kepler\cite{altintas2004kepler} and Taverna\cite{oinn2006taverna}.  

Pegasus \citep{singh2008workflow} is a more recent workflow management system for scientific applications.  It enables workflows to be executed both locally and on a cluster of computers in a simultaneous manner.  It has a rich set of APIs that allow the construction and representation of workflows as Directed Acyclic Graphs (DAGs).  It also has more advanced job scheduling and monitoring facilities than previous systems.  

One important capability that has been added as a functionality to some of these systems (see \cite{yuan2012data}) is the ability to store the computations of intermediate datasets with the purpose of optimizing the computation time of future workflows that are to be computed by the system and that make use of those intermediate datasets. For a scientific workflow system, storing all the intermediate data generated during workflow executions may cause high storage cost.  On the contrary, if we delete all the intermediate data and regenerate them every time when ever needed, the computation cost of the system may also be very high. Because of that, good tradeoffs need to be found to solve the problem.  They are sometimes achieved by constraining the amount of storage space available to the system, and other times by assigning cost to both storage space and computation time and designing decision algorithms that optimize for that cost. 

The scale of computations have been growing with time, and the ability of the systems cited above to process large amounts of data and to execute the placement of task execution on a distributed environment is still very limited.  Orthogonal to the development of workflow management systems, many distributed systems have been designed and developed to meet the growing demands of computation.  One of such systems is Apache Hadoop, which is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. \cite{islam2012oozie} introduced Apache Oozie, which is Apache Hadoop's workflow and scheduling system.  Its workflow definition API rivals that of Pegasus, while it takes advantage of the superior scalability of Hadoop. Unfortunately, it does not provide any capability to optimize the computation of workflows by saving the output of intermediate datasets in the hope of skipping the computation of future actions submitted to the system.

\section{Our Contributions}
The main contribution of our work is the design and implementation of Pingo, a system for the management of computations and storage of scientific workflows with constrained storage space. Our system has a commonality with previous systems in that it stores intermediate datasets produced by workflows with the purpose of skipping the computations of future workflows.  In order to decide which datasets should be kept or not, we have opted for the  approach of constraining the storage space available to the system, keeping datasets that will optimize the computational time of workflows under such constraint. But differing from previous systems, our system does it in a "reactive way".  That is, most other systems decide which intermediate datasets to keep on storage with foreknowledge of the definition of the future workflows that will be submitted to the system.  In our case, we analyze the history of workflows already submitted to the system in order to determine which datasets might be important to keep for the future. That difference in design makes it suitable for fast-paced research environments where it is impossible for the researcher to foresee what are the next steps to take.

Another important contribution of our work is that, to the best of our knowledge, this is the first system bringing those computational optimization capabilities to the Hadoop ecosystem.  A consequential contribution of designing it with Hadoop in mind is that we have also designed it to be an scalable multi-user system.  There are other smaller contributions scattered throughout the report. For example, we propose a hashing mechanism as an efficient alternative to determining the provenance of datasets produced by actions.  

Our presentation of Pingo is divided as follows: Chapter \ref{chap:foundational} contains a throughout explanation of the tradeoffs and balancing acts that need to be addressed (or at least considered) when designing a system that promises a functionality similar to the one we are attempting in our project. Chapter \ref{chap:implementation} provides a detailed description of the implementation of the system.  A central section of the chapter is the presentation of two families of decision algorithms that determine which intermediate datasets should be kept in storage at any given time. In Chapter \ref{chap:evaluation} I propose a methodology to evaluate the performance of the decision algorithms proposed in \rec{chap:implementation}.  I also report on the actual results of evaluating Pingo using such methodology.  Finally, in Chapter \ref{chap:future} I talk about the new possibilities of research that can improve and add to the functionality of the system.

