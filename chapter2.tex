\chapter{FOUNDATIONAL DESIGN PRINCIPLES OF THE SYSTEM}
\label{chap:foundational}
This chapter discusses the design of the \textbf{Pingo} system and the reasons behind it.  I have organized the discussion from a functional standpoint, discussing the design as it relates to the main functionalities of the system, the most essential functionality being the ability to receive the submission of workflows of actions and to carry out the computations represented by such actions.

\section{Skipping unnecessary computations}
If a user submits the description of an action $A$ to be computed, but the system has previously computed an action $B$ that produced the exact same output that action $A$ will produce when computed, then action $A$ does not need to be computed.  The output of action $B$ can be returned immediately as the output of action $A$.  

In order to achieve such functionality, there must be an equivalence relation between actions $A$ and $B$. This equivalence relation is not only restricted to the description of the actions provided by the user, but also to the place of the action in the workflow, since the output of an action is determined by both the definition of the computations represented by the action as well as the input to that action from previously computed actions. To define more precisely the equivalence relation between two actions, we need to first precisely define what is an \textbf{Action} and what is a \textbf{Workflow}.

\subsection{Actions}
The way we define what an \textbf{action} is will dictate the possibility of designing practical functionality that will allow us to optimize the time spent computing a workflow. 

Simplifying things, we can think of an action as a pure function $f:A \rightarrow B$.  Suppose that we are asked to compute $f(a)$.  If we have previously computed $b=g(c)$, and we can determine that $g=f$ and that $c=a$, then we can skip the computation of $f(a)$ (if it is that we still have $b$ among us.  Determining if $a$ and $c$ are equal to each other is trivial, but it can be time consuming if the inputs are big.  Determining if $f$ and $g$ are equivalent is more difficult and has deep theoretical ramifications (it is computationally impossible in the most broad sense, since the Halting Problem can be reduced to this problem).

But comparing our actions to pure mathematical functions is not completely accurate.  \cite{wiki:pure_function} says that a pure function is a function that "always evaluates the same result value given the same argument values.  The function result cannot depend on any hidden information or state that may change while program execution proceeds." In some cases, as we will see in the next chapter, there will be nothing stopping our actions from reading values from the environment, which in many cases is not under the control of the \textbf{Pingo} system.

Because of these difficulties, we need to devise a more practical approach to determine the equivalency between two actions.  That approach needs to be derived from a closer look to the functionality that we want our ideal system to embody.  The system will receive descriptions of actions as structured text.  Those descriptions are simply a collection of parameters such as: path to input folders, path to the executable (or source code) of the action, extra input parameters, etc.  The system uses that description to execute a corresponding computation which will produce an output as a file (or as a collection of files). We will be happy if our measure of equivalency between two actions satisfies the following two simple properties:

\begin{enumerate}
\item If two action descriptions produce the same output \textbf{when executed in an ideal controlled environment}, they should be considered equivalent.
\item If two action descriptions produce different output, they are inequivalent.
\end{enumerate}

Both properties are challenging to carry out in a real setting. The way we have worded the first property allows for the possibility of partial success that can be assessed using accuracy measures.  As we have mentioned before, achieving 100\% accuracy is theoretically impossible, and achieving high accuracy is challenging, since there are infinitely many ways to express two equivalent computations.  In devising a measure of equivalency between two actions, we will be happy if it can achieve consistent accuracy in at least the most common use cases.

The second property must be satisfied all the time for our measure of equivalency to be considered usable. Still, as will be described later in the chapter, it is convenient to interpret the phrase "all the time" not in a mathematically strict sense, but only in a practical sense.  Another challenge in order to satisfy the second property is that computations performed by actions might not be \textbf{pure functions}. (For a more throughout discussion on pure functions, see the discussion of \cite{jones2003haskell} on the Haskell programming language). If the function output depends on hidden information or state that may change while program execution proceeds, or between different executions of the program (i.e.: "randomization" or access to outside environmental variables), then there is no guarantee that the measure of equivalence will satisfy Property 2.  Because of that, the end user should have the ultimate responsibility of determining if a task should be recomputed even if there is present in the system an output dataset of an equivalent task.  If the end user thinks that the action description of an Action $A$ to be submitted to the system might produce a different output to that of an existing output of an equivalent previously submitted action $B$, then the user must let the system know that action $A$'s computation must be executed.

The description of an action submitted to the system becomes very relevant, since it is the starting point to determine equivalence between actions. A description schema that is very descriptive (pardon the redundancy) provides more elements to determine the equivalence between two actions.  As such it has the intended consequence of increasing the percentage of actions that can be determined to be equivalent. On the other hand, a very descriptive schema is burdensome to the final user, since in practice it causes the end user to spend more time writing the descriptions of the actions to be submitted to the system.  A good compromise needs to be found to this two conflicting aspects.

\subsection{Workflows}
In essence, a \textbf{workflow} is a directed acyclic graph (DAG) where nodes correspond to actions or original datasets (datasets not derived from the computation of an action) and a directed edge from a node $a$ to a node $b$ means that the output of action $A$ is used as as input by action $B$.  A topological sort of the workflow dictates a possible order of execution of the computations of the actions: actions with no parents, or actions whose parents' output we already know, can start executing whenever computational resources are available. The fact that the workflow is a DAG (i.e.: has no cycles) guarantees that all actions in the workflow will be computed at some point in time, given that there are no malformed computations or failures.  The discussion of how to handle failures belongs to the next chapter.

Before discussing more serious matters regarding workflows, we need to agree on some notational conventions.  We identify actions with the lower letter $a$ ($a_1$, $a_2$, ..., $a_n$); and original datasets with the lower letter $o$, and derived datasets with the lower letter $d$.  We also use functional notation to represent actions outputs from inputs.  For example, if action $a_1$ takes as inputs original dataset $o_1$, derived dataset $d_2$ and the output of action $a_3$ on original dataset $o_2$, we represent action $a_1$'s output $d_1$ as: $d_1 = a_1(o_1, d_2, a_3(o_2))$.

\subsubsection{Equivalence of actions in the workflow setting}
Consider the previous example of the definition of action $a_1$. What strategy can be devised to efficiently determine if dataset $d_1$ is in storage so that without having to calculate all the computations defined by the workflow where $a_1$ is? As a starting point, for every dataset $d_i$ kept in storage, we need to keep some accounting with the definition of the workflow that produced $d_i$.  Then we would need to search over those definitions until we find one that matches the current workflow submitted to the system.  

In order to analyze the running time complexity of such problem, let's look at algorithm described in Algorithm \ref{alg:dataset_search}. The algorithm takes as input action $a$ and hypothetical procedure $E$, whose job is to compare two action descriptions for equivalency, returning \textbf{true} if they are equivalent, and \textbf{false} otherwise.


\begin{algorithm}
\begin{singlespace}
\caption{Naive dataset search algorithm:}
\label{alg:dataset_search}
\begin{algorithmic}[1]
\Procedure{DatasetSearch}{$a, E$}
	\For{dataset $d_i$ in storage}  
		\State $a_i \gets action(d_i)$ \Comment{$a_i$ is action that outputted $d_i$}
		\State $Q \gets queue()$, $P \gets queue()$
		\State $Q.add(d_i)$, $P.add(a)$
		\State $areEqual \gets True$
		\While{Q \textbf{not} empty \textbf{and} P \textbf{not} empty}
			\State $q \gets pop(Q)$, $p \gets pop(P)$
			
			\If {E(q, p)}
				\State $Q.addAll(parents(q))$ \Comment{parents of $q$ in the workflow}
				\State $P.addAll(parents(p))$ \Comment{parents of $p$ in the workflow}
			\Else
				\State $areEqual \gets False$
				\State \textbf{Break}
			\EndIf
			
		\EndWhile
		\If {$areEqual$ \textbf{and} $Q.size = 0$ \textbf{and} $P.size = 0$}
			\Return $d_i$
		\EndIf
	\EndFor
	\Return \textbf{None}
\EndProcedure
\end{algorithmic}
\end{singlespace}
\end{algorithm}

The algorithm goes over each dataset $d_i$ in storage, and uses a simple Breadth First Search (BFS) strategy to compare the DAGs induced by the ancestors of both $a$ and $d_i$.  If it finds a dataset $d_i$ for whose DAG is equivalent to the DAG that produced $a$, it returns $d_i$, otherwise, it returns $None$.  Notice that for simplicity, the algorithm assumes that parent actions of an action are given to the corresponding queue in a correct order, so that when it pops them from the queues to compare them, they correspond to each other.  The next chapter (the implementation chapter) discusses how that assumption is valid for some action types, but not for others.

To analyze the running time of Algorithm \ref{alg:dataset_search}, let $m$ be the number of datasets in storage, and let $M$ and $N$ be the number of nodes and of edges of the submitted workflow.  The running time of the algorithm would then be $O(m (M + N))$.  Indexing the metadata of the stored datasets in a clever way, we can reduce that to a running time of $O(t (M + N))$ with $t << m$.

The indexing approach can be problematic to implement for at least two reasons: First, $t$ will increase without bounds as the system ages. The space occupied by the index of the metadata will also increase, making it difficult to keep it all in memory for fast computations. It is true that the index can be purged from time to time so that it only keeps relevant datasets in it, but it would represent added complexity that needs to be implemented.  The second reason is that indexing the metadata of a graph datastructure is not a trivial task. (\cite{simmhan2005survey})

\subsubsection{The hashing alternative}
\label{sec:encryption}
Another strategy that can be used to determine if an action's output already exists in the storage system is \textbf{hashing}.  Given an action node $a_i$ in a workflow, it is possible to recursively compose the description of action $a_i$ together with the descriptions of its parent actions to produce a hash value that "uniquely" identifies dataset $d_i$ produced by action $a_i$ as output; then compare that hash value with the hash values corresponding to the actions of the datasets stored in the system in order to find an equivalent dataset.

An algorithm that implements the ideas described above is highly dependent on the semantics of the description of the actions.  Because of that, we leave its discussion to the next chapter.  But there are enough elements to arrive at certain conclusions regarding the viability of the approach.  Firstly, comparing the hash value of action $a_i$ to the hash values of the datasets in the system is an action that can be done in constant time if the datasets are properly indexed.  Secondly, since it is impossible to know before hand the set of keys that are going to be hashed, the possibility of perfect hashing is discarded, and at least an upper bound analysis of what would be the probability of a clash between the hash signatures of non-equivalent actions is in place.  

An exhaustive mathematical analysis goes beyond the scope of this dissertation, but we do can make some educated estimations.  Assume that $n$ is the number of bits of the hash signatures produced by the hashing algorithm, and that D is the number of datasets currently in storage.  The question to answer is: What is the probability of any two non-equivalent datasets of having the same hash value?  There are $2^n$ possible hash values.  Assuming that all of them have the same probability of occurring, then the probability that all the $N$ hash values are different, $1 - p(N)$ is:

\begin{multline}
	1 - p(N) = 1 \times \frac{2^n - 1}{2^n} \times \frac{2^n - 2}{2^n} \times . . . \times \frac{2^n - N + 1}{2^n} \\ 
	1 - p(N) = \frac{2^n \times 2^n - 1 \times . . . \times 2^n - N + 1}{2^{n^{N}}} \\ 
	1 - p(N) = \frac{2^n !}{2^{n{^N}} (2^n - N)!} \\
\end{multline}

Then $p(N)$ is the probability that at least two of the N hash values are the same.  For $n=80$ (SHA-1 function) and $N=1,000,000$, the probability of a collision is $4.135580766728708e-13$.  This number can be considered acceptable for most practical purposes, but if for some reason it becomes unacceptable, the mechanism is still valid if we add a verification step to the search process.  The first step finds all the datasets whose corresponding hash value is equal to the one of the action under consideration.  In the second step, we can compare the workflows that produced each of the candidate  datasets to the workflow of the submitted action to exactly determine if they are equivalent.  A good hash function allows for the assumption that the number of candidate datasets is 1.

\section{Bounded storage space}
Placing a constraint of the amount of space available to store intermediate datasets makes the problem more interesting. There are physical limitations and technical limitations that won't allow us to have unlimited space.  Therefore, at most moments in time, the system needs to decide which datasets to keep in storage and which datasets to delete. The system also has to carry out the execution of the decisions.

\subsection{The History of Workflows}
The concept of \textbf{History of Workflows} becomes useful if the constraint of bounded storage capacity is added to the system. Since the final objective is to optimize the time of computing future workflows, the decision system can be thought of as an optimization problem that tries to "guess" which datasets will be more relevant in the future. How to accurately predict which datasets are needed in the future is an open question that has multiple valid answers. Most of the potential valid strategies make use of the history of workflows submitted to the system up to that point.  Different strategies might need different things from the history of workflows. Because of that, there needs to be an API layer that provides varied ways to query the history of workflows to obtain a diverse set of statistics over specified \textbf{ranges of time} and at different \textbf{resolutions}.

\subsection{The Decision System}
\label{sec:decision_system}
Pingo's core functionality is the Decision System that determines which datasets need to remain and which datasets need to be deleted from storage.  The Decision System can either run each time a new workflow is submitted to the system, or it can run only when space occupied by stored datasets reaches a certain threshold of the total capacity.  The second option makes more sense, since it will be able to complain with the bounded storage constraint as well as the first option, but uses the computational resources less. The Decision System defines an interface that can be implemented by different decision algorithms to determine which datasets currently in storage need to be deleted. The interface takes the following elements as input:

\begin{enumerate}
\item The History of Workflows submitted to the system, $H = (W_1, W_2, ..., W_n)$, where each $W_i$ is a DAG. It is left to the algorithm to decide if it will use the entire history or only a subset of it.

\item The set $D$ of datasets currently stored in the file system. 

\item $s: D \to \mathtt{N} $, where $s(d_i)$ is the storage space in megabytes that dataset $d_i$ occupies in the file system.

\item $t: A \to \mathtt{N}$, where $t(a_i)$ is the computational time in seconds that it takes to action $a_i$ to compute its output $d_i$.  If action $a_i$ has been computed multiple times, the average of those times is reported.

\item $F$, the amount of space that we need to free on the file system.

\end{enumerate}

The algorithm outputs a set $B \subset D$ such that $\sum_{d \in B}{s(d)}$ is greater than or equal to $F$.  Set $B$ needs to be selected in such a way that there is no other subset $B'$ of $D$ of storage size equal or greater than $B$ such that if datasets of $B'$ instead of $B$ are removed from storage, the system would spend less time computing future workflow submissions. 