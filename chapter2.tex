\chapter{FOUNDATIONAL DESIGN PRINCIPLES OF THE SYSTEM}
\label{chap:foundational}
In this chapter we discuss some of the reasons for the design decisions taken when implementing the system's functionality.  We will discuss the design decisions as they relate to the main functionalities that we intend to implement.
\section{Skipping unnecessary computations}

The system will constantly be managing the computation of actions submitted to it by end-users. If a user submits the description of an action A to be computed, but the system has previously computed an action B that produced the exact same output that action A will produce when computed, then action A does not need to be computed.  The output of action B can be returned immediately as the output of action A.  

How can we achieve such functionality? How can we know that actions A and B are equivalent? How can we know that an output dataset that we have in our filesystem is the final output of an entire workflow of computations that has been submitted to the system, and that we can skip the computation of not just one action, but of an entire collection of actions?  In order to be able to answer these questions, we need to explore what are the best strategies to follow when defining what is an \textbf{Action} and what is a \textbf{Workflow}.

\subsection{Actions}

The way we define what an \textbf{action} is will dictate the possibility of designing practical functionality that will allow us to optimize the time spent computing a workflow. 

Simplifying things, we can think of an action as a pure function $f:A \rightarrow B$.  Suppose that we are asked to compute $f(a)$.  If we have previously computed $b=g(c)$, and we can determine that $g=f$ and that $c=a$, then we can skip the computation of $f(a)$ (if it is that we still have $b$ among us.  Determining if $a$ and $c$ are equal to each other is trivial, but it can be time consuming if the inputs are big.  Determining if $f$ and $g$ are equivalent is more difficult and has deep theoretical ramifications (it is computationally imposible in the most broad sense, since the Halting Problem can be reduced to this problem).

Also, comparing our actions to pure mathematical functions is not completely accurate.  \cite{wiki:pure_function} says that a pure function is a function that "always evaluates the same result value given the same argument values.  The function result cannot depend on any hidden information or state that may change while program execution proceeds." In some cases, as we will see in the next chapter, there will be nothing stopping our actions from reading values from environmental variables.

Because of these difficulties, we need to devise a more practical approach to determine the equivalency between two actions.  That approach needs to be derived from a closer look to the functionality that we want our ideal system to embody.  The system will receive descriptions of actions as structured text.  As Figure \ref{fig:hypothetical_inputs} shows, those descriptions are simply a collection of parameters such as: path to input folders, path to the executable (or source code) of the action, extra input parameters, etc.  The system uses that description to execute a corresponding computation which will produce an output as a file (or as a collection of files). We will be happy if our measure of equivalency satisfies the following two simple properties:

\begin{enumerate}
\item If two action descriptions will produce the same output \textbf{when executed in our controlled environment}, they should be considered equivalent.
\item If two action descriptions will produce different output, they are inequivalent.
\end{enumerate}

\begin{figure}

\begin{mdframed}
\begin{verbatim}

	{
	   input: ["/path/to/input1", "/path/to/input2"],
	   output: "/path/to/output",
	   executable: ["/path/to/executable"],
	   inputParameters: [ {key: "arg1", value: "Hello"}, 
	                                 {key: "arg2", value: "World!"}]
	} 

\end{verbatim}
\end{mdframed}

\caption{Hypothetical example of description of an action}
\label{fig:hypothetical_inputs}
\end{figure} 

Both properties are challenging to carry out in a real setting. The way we have worded the first property allows for the possibility of partial success that can be assessed using accuracy measures.  As we have mentioned before, achieving 100\% accuracy is theoretically impossible, and achieving high accuracy is challenging, since there are infinitely many ways to express two equivalent computations.  In devising a measure of equivalency between two actions, we will be happy if it can achieve solid accuracy in at least the most common use cases.

The second property must be satisfied all the time for our measure of equivalency to be considered usable. Still, we will see later in this chapter how we intend to interpret "all the time" not in a mathematically strict sense, but only in a practical sense.  Another challenge that we will have in order to satisfy the second property is that we still have to deal with the issue that the computation performed by that action might not be a \textbf{pure function}.  If the function output depends on hidden information or state that may change while program execution proceeds, or between different executions of the program (i.e.: "randomization" or access to outside environmental variables), then we cannot make a guarantee that our measure of equivalence will satisfy Property 2.  Because of that, we need to leave on the end user the ultimate responsibility of taking advantage of any computational gains our system could provide. If the end user thinks that the action description of an Action A that he is about to submit to the system might produce a different output to that of an existing output of a previously submitted action B that the system will flag as equivalent to action A, then the user must let the system know that action A computation cannot be skipped. 

The description of an action submitted to the system becomes very relevant, since is the starting point to determine equivalence between actions. We could enforce a descriptive schema that is very descriptive (pardon the redundancy), and in that way have more elements to determine the equivalence between two actions, and maybe improve the accuracy of Property 1 of our equivalence measure.  But at the same time we might be making the system burdensome to use to the final user, since they will have to invest considerable time writing the descriptions of the actions that they are submitting to the system.  A good balance needs to be found here.


\subsection{Workflows}
A \textbf{workflow} is essentially a directed acyclic graph (DAG) where nodes correspond to actions or original datasets (datasets not derived from the computation of an action) and a directed edge from a node $a$ to a node $b$ means that the output of action $A$ is used as as input by action $B$.  A topological sort of the workflow dictates a possible order of execution of the actions computations: actions with no parents, or actions whose parents' output we already know, can start executing whenever computational resources are available. The fact that the workflow is a DAG (i.e.: has no cycles) guarantees that all actions in the workflow will be computed at some point in time, given that there are no malformed computations or failures.  Handling failures is a discussion that we leave for the next chapter, when we describe the implementation of this chapter's ideas.

Before discussing more serious matters regarding workflows, we need to agree on some notational conventions.  We will identify actions with the lower letter $a$ ($a_1$, $a_2$, ..., $a_n$); and original datasets with the lower letter $o$, and derived datasets with the lower letter $d$.  We will also use functional notation to represent actions outputs an inputs.  If an action $a_1$ takes as input original dataset $o_1$, derived dataset $d_2$ and the output of action $a_3$ on original dataset $o_2$, we represent its output as follows: $d_1 = a_1(o_1, d_2, a_3(o_2))$.

\subsubsection{Equivalence of actions in the workflow setting}
Consider the previous example with the definition of action $a_1$. What strategy can we devise to efficiently determine if we have dataset $d_1$ in storage without having to calculate all the computations defined by the workflow where $a_1$ is? As a starting point, for every dataset $d_i$ kept in storage, we need to keep some accounting with the definition of the workflow that produced $d_i$.  Then we would need to search over those definitions until we find one that matches the current workflow submitted to the system.  

In order to analyze the running time complexity of such problem, let's look at algorithm described in Algorithm \ref{alg:dataset_search}. The algorithm takes as input action $a$ and hypothetical procedure $E$, whose job is to compare two action descriptions for equivalency, returning \textbf{true} if they are equivalent, and \textbf{false} otherwise.


\begin{algorithm}
\begin{singlespace}
\caption{Naive dataset search algorithm:}
\label{alg:dataset_search}
\begin{algorithmic}[1]
\Procedure{DatasetSearch}{$a, E$}
	\For{dataset $d_i$ in storage}  
		\State $a_i \gets action(d_i)$ \Comment{$a_i$ is action that outputted $d_i$}
		\State $Q \gets queue()$, $P \gets queue()$
		\State $Q.add(d_i)$, $P.add(a)$
		\State $areEqual \gets True$
		\While{Q \textbf{not} empty \textbf{and} P \textbf{not} empty}
			\State $q \gets pop(Q)$, $p \gets pop(P)$
			
			\If {E(q, p)}
				\State $Q.addAll(parents(q))$ \Comment{parents of $q$ in the workflow}
				\State $P.addAll(parents(p))$ \Comment{parents of $p$ in the workflow}
			\Else
				\State $areEqual \gets False$
				\State \textbf{Break}
			\EndIf
			
		\EndWhile
		\If {$areEqual$ \textbf{and} $Q.size = 0$ \textbf{and} $P.size = 0$}
			\Return $d_i$
		\EndIf
	\EndFor
	\Return \textbf{None}
\EndProcedure
\end{algorithmic}
\end{singlespace}
\end{algorithm}

The algorithm goes over each dataset $d_i$ in storage, and uses a simple Breadth First Search (BFS) strategy to compare the DAGs induced by the ancestors of both $a$ and $d_i$.  If it finds a dataset $d_i$ for whose DAG is equivalent to the DAG that produced $a$, it returns $d_i$, otherwise, it returns $None$.  Notice that for simplicity, the algorithm assumes that parent actions of an action are given to the corresponding queue in a correct order, so that when we pop them from the queues to compare them, they correspond to each other.  In the next chapter (the implementation chapter) we will see how that assumption is valid for some action types, but not for others.

To analyze the running time of this algorithm, let $m$ be the number of datasets in storage, and let $M$ and $N$ be the number of nodes and of edges of the submitted workflow.  The running time of the algorithm would then be $O(m (M + N))$.  If we index the metadata of our stored datasets in a clever way, we can reduce that to a running time of $O(t (M + N))$ with $t << m$.

But this approach will be problematic to implement for at least two reasons: First, $t$ will increase without bounds as the system ages. The space occupied by the index of the metadata will also increase, making it difficult to keep it all in memory for fast computations. The second reason is that indexing the metadata of a graph datastructure is not a trivial task.

\subsubsection{The encryption alternative}
Encryption is another strategy that can be used to determine if an action's output already exists in the storage system. Given an action node $a_i$ in a workflow, we can recursively compose the description of action $a_i$ together with the descriptions of its parent actions to produce a hash value that "uniquely" identifies the dataset $d_i$ produced by action $a_i$ as output.  We can then compare that hash value with the hash values corresponding to the actions of the datasets stored in the system in order to find an equivalent dataset.

An algorithm describing the above situation is highly dependent on the semantic of the description of the actions.  Because of that, we leave its discussion to the next chapter.  But we have enough elements as to arrive to certain conclusions regarding the viability of the approach.  Firstly, comparing the hash value of action $a_i$ to the hash values of the datasets in the system is an action that can be done in constant time if the datasets are properly indexed.  Secondly, since no cryptographic function is perfect, we need to analyze what would be the probability of a clash between the hash signatures of non-equivalent actions.  

An exhaustive mathematical analysis goes beyond the scope of this dissertation, but we do can make some educated estimations.  Assume that $n$ is the number of bits of the hash signatures produced by the encryption algorithm, and that D is the number of datasets currently in storage.  The question we want to answer is: What is the probability of any two of the datasets of having the same hash value?  There are $2^n$ possible hash values.  Assuming that all of them have the same probability of occurring, then the probability that all the $N$ hash values are different, $1 - p(N)$ is:

\begin{multline}
	1 - p(N) = 1 \times \frac{2^n - 1}{2^n} \times \frac{2^n - 2}{2^n} \times . . . \times \frac{2^n - N + 1}{2^n} \\ 
	1 - p(N) = \frac{2^n \times 2^n - 1 \times . . . \times 2^n - N + 1}{2^{n^{N}}} \\ 
	1 - p(N) = \frac{2^n !}{2^{n{^N}} (2^n - N)!} \\
\end{multline}

Then p(N) is the probability that at least two of the N hash values are the same.  For $n=80$ (SHA-1 function) and $N=1,000,000$, we have that the probability of a collision is $4.135580766728708e-13$.  This number can be considered acceptable for most practical purposes, but if for some reason it becomes unacceptable, the mechanism is still partially valid.  We only need to add a second step to the process.  In the first step we can still use the hash value of an action that is being submitted to the system to quickly find an already existing \textbf{tentative} dataset. Then we can compare the workflow that produced the tentative dataset to the workflow of the submitted action to exactly determine if they are equivalent.

\section{Bounded storage space}
Bounded storage space is the constraint that makes the problem we are trying to solve interesting. There are physical limitations and technical limitations that won't allow us to have unlimited space.  Therefore, at all moments in time, the system needs to decide which datasets to keep in storage and which datasets to delete.  The system also has to enforce its own decisions.

\subsection{The History of Workflows}
The concept of \textbf{History of Workflows} becomes useful once we add the constraint of bounded storage capacity to our system. Since we want to optimize the time of computing future workflows, the decision system is in reality an optimization problem that tries to "guess" which datasets will be more relevant in the future. What strategy should we use in order to optimize the computational time of future workflow submissions?  That is an open question that has multiple valid answers.  But whatever strategy we use to accurately predict which datasets might be needed in the future will need access to the history of workflows submitted to the system up to that point.  Because different strategies might need different things from the history of workflows, it is recommended to provide a flexible API that can be used to query the history of workflows to obtain a diverse set of statistics over specified periods of time and at different resolutions.

\subsection{The Decision System}
\label{sec:decision_system}
The core functionality of the system is the Decision System that determines which datasets need to remain and which datasets need to be deleted from the storage of the system.  The Decision System can be run each time a new workflow is submitted to the system, or it can be run only when space occupied by stored datasets reaches a certain threshold of the total capacity of the system. The second option makes more sense, so we will define our interface with that in mind. The Decision System defines an interface that can be implemented by different algorithms to determine which datasets currently in storage need to be deleted. The interface takes the following elements as input:

\begin{enumerate}
\item The History of Workflows submitted to the system, $H = (W_1, W_2, ..., W_n)$, where each $W_i$ is a DAG. It is left to the algorithm to decide if it will use the entire history or only a subset of it.

\item A set $D$ of datasets currently stored in the file system. 

\item $s: D \to \mathtt{N} $, where $s(d_i)$ is the storage space in megabytes that dataset $d_i$ occupies in the file system.

\item $t: A \to \mathtt{N}$, where $t(a_i)$ is the computational time in seconds that it takes to action $a_i$ to compute its output $d_i$.  If action $a_i$ has been computed multiple times, the average of those times is reported.

\item $F$, the amount of space that we need to free on the file system.

\end{enumerate}

The algorithm outputs a set $B \subset D$ such that $\sum_{d \in B}{s(d)}$ is greater than or equal to $F$.  Set $B$ needs to be selected in such a way that there is no other subset $B'$ of $D$ of storage size equal or greater than $B$ such that if we remove from storage datasets of $B'$ instead of $B$, we would spend less time computing our future workflows. 

\subsubsection{Literature Review of Related Problems}
\label{sec:lit_review_algorithms}
This section can be skipped without risking lack of understanding of the rest of the presentation, but for completeness sake we think that is necessary to review multiple efforts in the literature to solve problems similar to the one we have posed in Section \ref{sec:decision_system}. 

Is not surprising that none of the related problems have been defined in the same way we have defined ours, so we want to offer some commentary on which of them we think are more relevant to problem in \ref{sec:decision_system} and why.

\section{A multi-user multi-component system}