\chapter{THEORETICAL CONSIDERATIONS OF THE DATA REUSE PROBLEM}
\label{chap:foundational}
This chapter discusses the theoretical motivations behind data reuse systems.  It introduces the data reuse problem after providing necessary background. It also presents two families of algorithms that provide solutions to the problem. The discussion is organized from a functional standpoint, discussing the theoretical motivations and tradeoffs that drive the design as they relate to the desirable functions of a system that attempts to solve the data reuse problem.

\section{Skipping unnecessary computations}
If a user submits the description of an action $A$ to be computed, but the system has previously computed an action $B$ that produced the exact same output that action $A$ will produce when computed, then action $A$ does not need to be computed.  The output of action $B$ can be returned immediately as the output of action $A$.  

In order to achieve such functionality, there must be an equivalence relation between actions $A$ and $B$. This equivalence relation is not only restricted to the description of the actions provided by the user, but also to the place of the action in the workflow, since the output of an action is determined by both the definition of the computations represented by the action as well as the input to that action from previously computed actions. To define more precisely the equivalence relation between two actions, precise definitions of \textbf{Actions} and \textbf{Workflows} are needed. The discussion follows the definitions of the model introduced by \cite{zohrevandi2013bounded} in their work, with some slight variations in terminology, and some additions, such as the concept of History of Workflows.

\subsection{Actions}
An action can be thought of as a pure function $f:A \rightarrow B$.  Suppose that the system is asked to compute $f(a)$.  If the system has previously computed $b=g(c)$, and it can be determined that $g=f$ and that $c=a$, then the system can skip the computation of $f(a)$ (if it is that output $b$ has not been deleted).  Determining if $a$ and $c$ are equal to each other is trivial, but it can be time consuming if the inputs are big.  Determining if $f$ and $g$ are equivalent is in general undecidable \citep{turing1937computable}.

But comparing actions to pure mathematical functions is not completely accurate.  \cite{gifford1986integrating} say that a pure function is a function that is "referentially transparent: it does not cause side-effects, its value is not affected by side-effects, and it returns the same value each time it is evaluated." In other words, the function result cannot depend on any hidden or random information or state that may change while the function executes, or across two different executions of the function. In some cases, as next chapter shows, there will be nothing stopping actions from reading values from the outside environment, or from using randomization. Users can also submit binary files (instead of source code) for execution, or computations with third party library dependencies over which the system has no control.

Because of these difficulties, the system needs a practical approach to determine the equivalency between two actions.  Instead of looking into the source code (or executable code) of the actions to determine if they are equal or equivalent, it is more convenient to look into the configuration file that corresponds to the actions. Those configurations are simply a collection of parameters such as: path to input folders, path to the executable (or source code) of the action, extra input parameters, etc. 

A measure of equivalency between two actions should satisfy the following two simple properties. The description (or configuration parameters) of an action submitted to the system becomes then very relevant, since it is the starting point to determine equivalence between actions. 

\begin{enumerate}
\item If two action descriptions represent actions that produce the same output \textbf{when executed in an ideal controlled environment}, they should be considered equivalent.
\item If two action descriptions represent actions that produce different output, they are inequivalent.
\end{enumerate}

The language in the second property is absolute, since the property must be satisfied at all times for the measure of equivalency to be considered usable.  As we have seen, a challenge of the second property is that computations performed by actions might not be \textbf{pure functions}.  Because of that, if the end user thinks that the execution of an Action $A$  might produce a different output to that of an existing output of an equivalent previously submitted action $B$, then the user must let the system know that no optimizations should be applied on action $A$ and that its computation must be executed. 

\subsection{Workflows}
In essence, a \textbf{workflow} is a DAG where nodes correspond to \textbf{actions} or to \textbf{original datasets} (datasets not derived from the computation of an action). A directed edge from a node $a$ to a node $b$ means that the output of action $A$ is used as input by action $B$.  Actions are identified with the lower letter $a$ ($a_1$, $a_2$, ..., $a_n$), original datasets with the lower letter $o$, and derived datasets with the lower letter $d$.  Functional notation is used to represent actions outputs from inputs.  For example, if action $a_1$ takes as inputs original dataset $o_1$, derived dataset $d_2$ and the output of action $a_3$ on original dataset $o_2$, action $a_1$'s output $d_1$ is represented as: $d_1 = a_1(o_1, d_2, a_3(o_2))$.

A topological sort of the workflow dictates a possible order of execution of the computations of the actions: actions with no parents, or actions whose parents' output are already known, can start executing whenever computational resources are available. The fact that the workflow is a DAG guarantees that all actions in the workflow will be computed at some point in time, given that there are no malformed computations or failures.  Chapter \ref{chap:implementation} has a complete discussion of failure handling.

\subsubsection{Equivalence of actions in the workflow setting}
Consider the previous example of the definition of action $a_1$. What strategy can be devised to efficiently determine if its corresponding output dataset $d_1$ is already in storage? As a starting point, for every dataset $d_i$ kept in storage, the system needs to keep some accounting, including the definition of the workflow that produced $d_i$.  Then it is possible to search over those definitions to find one that matches the current workflow submitted to the system.  

In order to analyze the running time complexity of such problem, let's look at a naive search algorithm (Algorithm \ref{alg:dataset_search}). The algorithm takes as input action $a$ and hypothetical procedure $E$, whose job is to compare two action descriptions for equivalency, returning \textbf{true} if they are equivalent, and \textbf{false} otherwise.


\begin{algorithm}
\begin{singlespace}
\caption{Naive dataset search algorithm:}
\label{alg:dataset_search}
\begin{algorithmic}[1]
\Procedure{DatasetSearch}{$a, E$}
	\For{dataset $d_i$ in storage}  
		\State $a_i \gets action(d_i)$ \Comment{$a_i$ is action that outputted $d_i$}
		\State $Q \gets queue()$, $P \gets queue()$
		\State $Q.add(d_i)$, $P.add(a)$
		\State $areEqual \gets True$
		\While{Q \textbf{not} empty \textbf{and} P \textbf{not} empty}
			\State $q \gets pop(Q)$, $p \gets pop(P)$
			
			\If {E(q, p)}
				\State $Q.addAll(parents(q))$ \Comment{parents of $q$ in the workflow}
				\State $P.addAll(parents(p))$ \Comment{parents of $p$ in the workflow}
			\Else
				\State $areEqual \gets False$
				\State \textbf{Break}
			\EndIf
			
		\EndWhile
		\If {$areEqual$ \textbf{and} $Q.size = 0$ \textbf{and} $P.size = 0$}
			\Return $d_i$
		\EndIf
	\EndFor
	\Return \textbf{None}
\EndProcedure
\end{algorithmic}
\end{singlespace}
\end{algorithm}

The algorithm goes over each dataset $d_i$ in storage, and uses a simple Breadth First Search (BFS) strategy to compare the DAGs induced by the ancestors of both actions $a$ and $a_i$ (where $a_i$ is the action whose output is $d_i$).  If it finds an dataset $a'$ whose DAG is equivalent to the DAG that produced $a$, it returns the output dataset of action $a'$, otherwise, it returns $None$.  Notice that for simplicity, the algorithm assumes that parent actions of an action are given to the corresponding queue in a correct order, so that when it pops them from the queue to compare them, they correspond to each other.  Chapter \ref{chap:implementation} discusses how that assumption is valid for some action types, but not for others.

To analyze the running time of Algorithm \ref{alg:dataset_search}, let $m$ be the number of datasets in storage, and let $M$ and $N$ be the number of nodes and of edges of the submitted workflow.  The running time of the algorithm would then be $O(m (M + N))$.

\subsubsection{The hashing alternative}
\label{sec:encryption}
Another strategy that can be used to determine if an action's output already exists in the storage system is \textbf{hashing}.  Given an action node $a_i$ in a workflow, it is possible to recursively compose the description of action $a_i$ together with the descriptions of its parent actions to produce a hash value that "uniquely" identifies dataset $d_i$ produced by action $a_i$ as output; then compare that hash value with the hash values corresponding to the actions of the datasets stored in the system in order to find an equivalent dataset. This idea is similar to the concept of a Merkle tree \citep{merkle1990certified}. Merkle trees are binary (or more generally, k-ary) trees where each non-leaf node is the hash of the concatenation of its children, and each leaf node is the hash of one file from the collection.  In general for a workflow (see Figure \ref{fig:hash_structure}) its hash structure is not a tree but a DAG.  Also, for each node in the hash structure, its corresponding hash value is not just he concatenation of its parents' hash value but also the configuration parameters of the action.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{workflow_hash_structure}
\caption{Hypothetical workflow and its corresponding hash structure}
\label{fig:hash_structure}
\end{figure}

An algorithm that implements the ideas described above is highly dependent on the semantics of the description of the actions.  Because of that, we leave its discussion to Chapter \ref{chap:implementation}.  But there are enough elements that allow us to arrive at certain conclusions regarding the viability of the approach.  Firstly, comparing the hash value of action $a_i$ to the hash values of actions whose datasets are in storage is something that can be done in constant time with the proper indexing structure.  Secondly, since it is impossible to know beforehand the set of keys that are going to be hashed, the possibility of perfect hashing is discarded, and at least a lower bound analysis of what would be the probability of a clash between the hash signatures of non-equivalent actions is in place.  

Assume that $n$ is the number of bits of the hash signatures produced by the hashing algorithm, and that N is the number of datasets currently in storage.  First, I provide a lower bound of the probability of a single collision on the leafs. There are $2^n$ possible hash values.  Assuming that all of them have the same probability of occurring (see \textbf{random oracle model} by \cite{bellare1993random}), then the probability that all the $N$ hash values are different, $1 - p(N)$ is:

\begin{multline}
	1 - p(N) = 1 \times \frac{2^n - 1}{2^n} \times \frac{2^n - 2}{2^n} \times . . . \times \frac{2^n - N + 1}{2^n} \\ 
	1 - p(N) = \frac{2^n \times 2^n - 1 \times . . . \times 2^n - N + 1}{2^{n^{N}}} \\ 
	1 - p(N) = \frac{2^n !}{2^{n{^N}} (2^n - N)!} \\
\end{multline}

Then $p(N)$ is the probability that at least two of the N hash values are the same.  For $n=80$ (SHA-1 function) and $N=1,000,000$, the probability of a collision is $4.135580766728708e-13$. 

Now, take for example, two simple binary trees with two leaves.  Let tree 1's leaves be $A$ and $B$, and tree 2's leaves $X$ and $Y$.   The probability that the tree hashes collide at the root is the probability that $(h(A), h(B)) \neq (h(X), h(Y))  \wedge h(h(A), h(B)) = h(h(X), h(Y))$ plus the probability that $(h(A), h(B)) = (h(X), h(Y))$.  The probability of the second part is equal to $p(N)^2$, and the probability of the first is $p(N) * (1 - p(N)^2)$. The total probability becomes $p(N) + p(N)^2 - p(N)^2 * p(N)$, which is slightly greater than $p(N)$, the probability of a single collision.  As the depth of the tree increases, the probability of a collision on the root also increases slightly. \cite{coron2005merkle} show under which conditions collision resistance of the compression function $f$ is sufficient to obtain collision resistance of the hash function of a Merkle tree. Their work shows that a compression function like $SHA2$ is sufficient. At any rate I introduce a verification step to the search process.  The first step of the search process finds all the datasets whose corresponding workflow's hash value is equal to the one of the workflow under consideration. The second step compares the workflows themselves to determine if they are equivalent.

\section{Bounded storage space}
Placing a constraint on the amount of space available to store intermediate datasets makes the problem more interesting. There are physical limitations and technical limitations that won't allow us to have unlimited space.  Therefore, at most moments in time, the system needs to decide which datasets to keep in storage and which datasets to delete. The system also has enforce the decisions.

\subsection{The History of Workflows}
The concept of \textbf{History of Workflows} becomes useful when the constraint of bounded storage capacity is added to the system. Since the final objective is to optimize the time of computing future workflows, the decision system can be thought of as an optimization problem that tries to "guess" which datasets will be more relevant in the future. How to accurately predict which datasets are needed in the future is an open question that has multiple valid answers. Most of the potential valid strategies make use of the history of workflows submitted to the system up to that point.  Different strategies might need different information from the history of workflows. Because of that, there needs to be an API layer to query the history of workflows to obtain a diverse set of statistics over specified \textbf{ranges of time} and at different \textbf{resolutions}.

\subsection{The Data Reuse Problem}
\label{sec:decision_system}
A core functionality of any workflow management system that wants to tackle the data reuse problem would be its decision system.  The decision system determines which datasets need to remain and which datasets need to be deleted from storage.  It can either run each time a new workflow is submitted to the system, or it can run only when space occupied by stored datasets reaches a certain threshold of the total capacity.  The second option makes more sense, since under it the system remains in compliance with the bounded storage constraint, but uses less computational resources. The decision system defines an interface that can be implemented by different decision algorithms to determine which datasets currently in storage need to be deleted. Let $D$ be the set of datasets currently stored in the file system. The goal of the algorithms is to output a set $B \subset D$ of datasets such that $\sum_{d \in B}{s(d)}$ is greater than or equal to $F$.  Set $B$ needs to be selected in such a way that there is no other subset $B'$ of $D$ of storage size equal or greater than $B$ such that if datasets of $B'$ instead of $B$ are removed from storage, the system would spend less time computing future workflow submissions. 

The interface takes the following elements as input:

\begin{enumerate}
\item The History of Workflows submitted to the system, $H = (W_1, W_2, ..., W_n)$, where each $W_i$ is a DAG. It is left to the algorithm to decide if it will use the entire history or only a subset of it.

\item The set $D$ of datasets currently stored in the file system. 

\item $s: D \to \mathtt{N} $, where $s(d_i)$ is the storage space that dataset $d_i$ occupies in the file system.

\item $c_H: D \to \mathtt{N}$, where $c_H(d_i)$ is the number of times that dataset $d_i$ appears in History of Workflows $H$.

\item $t: A \to \mathtt{N}$, where $t(a_i)$ is the computational time that it takes action $a_i$ to compute its output $d_i$.  If action $a_i$ has been computed multiple times, the average of those times is reported.

\item $F$, the amount of space to free on the file system.

\end{enumerate}

\subsection{Two families of algorithms}
Similar versions of the problem presented above have been studied in other areas of Computer Science.  For example similar problems appear in the operating systems literature in the areas of memory caches \citep{smith1982cache} and scheduling (for a survey of algorithms, see the work of \cite{ramamritham1994scheduling}). But I have found the nature of the problems in the field of web caching and prefetching to be the most similar to the data reuse problem.  Web catching and prefetching techniques play a key role in improving web performance by keeping web objects that are likely to be visited in the near future closer to the client. They exploit both the temporal and spatial locality for predicting revisiting objects. In their context, spatial locality is modeled as a graph of objects. That characteristic makes their solutions more suitable for application to the data reuse problem in workflows, since one can think of the DAG that defines the workflows as a way of representing "spatial locality".  For excellent surveys of web caching and prefetching algorithms, see the works of \cite{wang1999survey} and \cite{ali2011survey}.

This section introduces two families of algorithms that solve the problem introduced in Section \ref{sec:decision_system} with varying degrees of success. They use the most basic ideas from the caching literature referred above. Chapter \ref{chap:evaluation} introduces a methodology to evaluate the algorithms' performance from a practical standpoint.

\subsubsection{Least Valuable Algorithm Family}
This is a very simple algorithm that represents a big family of possible algorithms that can be implemented.  The idea is to retain in storage the datasets with the most valuable datasets, according to some evaluation metric. Implementations of this base algorithm define their own evaluation metrics.  For example, Algorithm \ref{alg:most_commonly_used_decision} defines value as the number of times the dataset is used throughout the history of workflows times the time it takes to compute such dataset. That is a very simple definition of value, but provides the most straightforward implementation.  Algorithm \ref{alg:most_commonly_used_decision} is a good baseline to compare against.

\begin{algorithm}
\begin{singlespace}
\caption{Least-Valuable-Datasets Algorithm}
\label{alg:most_commonly_used_decision}
\begin{algorithmic}[1]
\Procedure{LeastValuable}{$H, D, c_H, s, t, F$}
	\State $datasetValues \gets List()$
	\For{$d_i$ in $H$}
		\State $datasetValues.append((d_i, c_H(d_i) * t(d_i)))$
	\EndFor
	\State $sortedDatasetValues \gets sortByValue(datasetValues)$
	\State $spaceFreed \gets 0$
	\State $toDelete \gets List()$
	\For{$d$, $value$ in $sortedDatasetValues$}
		\If{$spaceFreed \geq F$}
			\State \textbf{break}
		\EndIf	
		\State $toDelete.append(dataset)$
		\State $spaceFred \gets spaceFreed + s(d)$
	\EndFor
	
	\State \textbf{return} $toDelete$
\EndProcedure
\end{algorithmic}
\end{singlespace}
\end{algorithm}


\begin{algorithm}
\begin{singlespace}
\caption{Adaptive Least-Valuable-Datasets Algorithm}
\label{alg:adaptive_most_commonly_used_decision}
\begin{algorithmic}[1]
\Procedure{AdaptiveLeastValuable}{$H, D, c_H, s, t, F$}
	\State $recencyList \gets List()$
	\State $n \gets length(H)$
	\For{$i$ in from 1 to $n$}
		\For{dataset $d$ in $W_i$}
			\State $W_j \gets$ previous workflow where d was used.
			\If{$W_j$ \textbf{is not} $Null$}
				\State $recencyList.append(i - j)$
			\EndIf
		\EndFor
			
	\EndFor
	\State $\mu \gets mean(recencyList)$
	\State $\sigma \gets std(recencyList)$
	\State $SH \gets H.subList(max(0, int(n - \mu - 2\sigma)), n + 1)$
	\State \textbf{return} $LeastValuable(SH, D, c_{SH}, s, t, F)$

\EndProcedure
\end{algorithmic}
\end{singlespace}
\end{algorithm}

\subsubsection{Simple Adaptive Algorithm Family}
Algorithms that belong to the $Least Valuable Algorithm$ family are a good starting point, but they have the weakness that they use the entire history of workflows to compute the value of datasets. That strategy might be out of touch with reality, especially in fast-paced research settings where more recent datasets are more likely to be reused than less recent ones. One important question to ask is: How far back into the history of workflows do we need to look to determine the value of datasets? I propose a simple adaptive algorithm, described in Algorithm \ref{alg:adaptive_most_commonly_used_decision}.  Its strategy is that for each workflow, for each dataset produced in that workflow, the algorithm samples how many workflows into the past one has to look back to find the previous time that such dataset was needed (not necessarily computed).  After building that sample of intervals, the algorithm finds its mean ($\mu$) and standard deviation ($\sigma$).  By passing to the $LeastValuable$ procedure a sub-history of workflows that goes back $\mu - 2\sigma$ steps into the past, it guarantees that the $LeastValuable$ procedure receives a history of workflows that although not complete, its statistics reflect more accurately the current usage trends of the system.
