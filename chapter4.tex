\chapter{Evaluation methodology of the decision algorithms and results}
\label{chap:evaluation}
This chapter proposes an evaluation methodology for the decision algorithms of Pingo.  It also reports on the evaluation of the two different kinds of algorithms that we have implemented.

It is almost impossible to obtain enough real world workflows data to do an statistically meaningful evaluation of the system. A good alternative is to use a probabilistic generator of workflows with the flexibility to adjust parameters to create different types of workloads.  That strategy allows for a more fine-grained evaluation and comparison of how the algorithms behave under different types of workloads.

\section{Evaluation Methodology}
The strategy to evaluate the the Decision System is:
\begin{enumerate}
\item Probabilistically generate a history $H$ of workflows.
\item Compute the \textbf{ideal execution time} of history $H$
\item Submit the history $H$ to Pingo for computation and record the \textbf{actual execution time}
\item Compare the ideal execution time with the real execution time.  The higher the ratio of $actual/ideal$, the best the algorithm.
\end{enumerate}
\subsection{Workflows generator}
The workflow generator creates sequences of workflows in a probabilistic way given certain parameters. All the actions are Java $COMMAND\_LINE$ actions that take as parameters the size of an output in megabytes and the time of execution of that action in seconds.  The task of those actions is to output a file with random contents and with size as given by the parameters and to stay in a for loop without returning for the amount of seconds specified by the second parameter.

The sequence generator is composed of two probabilistic generators that work together to produce the history of of workflows.  See Appendix \ref{app:workflows_generator_user_guide} for information on how to use the generator.  

\subsubsection{The Action Generator}
The first of the generators is the \textbf{Action Generator}.  It takes as input the number of actions to generate and the mean and variance parameters of two normal distributions, one for the size of outputs and another one for the computational time of the action.  It generates a list of actions, each one with a unique id and its corresponding randomly generated parameters. The workflow generator uses this list of actions as a pool of actions from where to select the actions to compose the workflows.

\subsubsection{The Workflow Generator}
The \textbf{Workflow Generator} is a more complex piece of computation.  Its purpose is to generate a workflow (DAG) selecting nodes from the pool of actions created by the Action Generator.  Appendix \ref{app:workflow_generator_implementation} has a Python implementation of the algorithm.  Roughly, the algorithm does the following:
\begin{enumerate}
\item Selects $n$ nodes from the composition of all previous workflows up to that point in the sequence of workflows. It takes good care that if any pair of nodes nodes $a$, $b$ among the $n$ nodes are related between each other (antecesor/succesor) relationship, then the nodes in between them are also included among the $n$ nodes.

\item It randomly selects $workflow\_size - n$ new actions from the pool of actions that are not nodes in the DAGs of the workflows already generated..

\item Create two normal distributions with parameters provided in configuration.  Call one distribution the $childrenDist$ and the other one $parentDist$. For each action $a$ selected to be part of the new workflow, if action $a$ was selected from previous workflows, use $childrenDist$ to generate the number of children that this action will have.  Otherwise if action $a$ is a new action, use, $childrenDist$ and $parentDist$ to generate the number of children and the number of parents that this action will have, respectively.

\item Use a greedy algorithm to create a directed acyclic graph that satisfies the constraints of number of children and number of parents a node will have in the best possible way and return the corresponding workflow.
\end{enumerate}

The parameters used to define the structure of the DAGs are good enough to produce most of the varieties of histories of workflows to imagine. For example, Figure \ref{fig:dag_tree} shows how to generate tree DAGs with very high probability if the number of parents that an action can have is restricted to only one.  Figure \ref{fig:dag_complex} shows how to generate DAGs with more complex dependency relationships between nodes.  Last, Figure \ref{fig:dag_variety} shows an example of the varieties of graph produced when the variance of the distributions is increased.
\begin{figure}
\centering
\includegraphics[scale = 0.5]{dag_tree}
\caption{Example of a tree workflow that is produced with parameters: nb\_children.mean = 2, nb\_children.std =  1, nb\_parents.mean = 1, nb\_parents.std = 0.00001.}
\label{fig:dag_tree}
\end{figure}
\begin{figure}
\centering
\includegraphics[scale = 0.5]{dag_interwinded}
\caption{Example of DAG that is produced with parameters: nb\_children.mean = 2.1, nb\_children.std = 0.0001, nb\_parents.mean = 2.1, nb\_parents.std = 0.0001.}
\label{fig:dag_complex}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale = 0.5]{variance_all}
\caption{Example of four DAGs produced with parameters: nb\_children.mean = 2.1, nb\_children.std = 4.5, nb\_parents.mean = 2.1, nb\_parents.std = 4.5.}
\label{fig:dag_variety}
\end{figure}


\subsection{Ideal Execution Time calculation}
Most of the research done in the area of scientific workflows becomes relevant and applicable, not in the context of the \textbf{Decision System} of Pingo, but in the context of the metrics to evaluate the \textbf{Decision System}.  Most of the previous research has focused in finding an optimal solution to the problem of scientific workflows with constrained space, assuming that we know the entire history of the workflows that will be submitted to the system from the beginning.  In Pingo, the \textbf{Decision System} does not know the end from the beginning.  Instead, it only uses the previous workflows submitted to the system to predict how the future workflows might look like.  This different approach makes sense in fast-paced research settings where researchers don't know from the start the exact process (and hence the workflow) that they will follow in their research.

For evaluation purposes it is possible to know the end from the beginning and all the research from Section \ref{chap:introduction} becomes more directly relevant to our problem.  As a future endeavor, it would be good to do a more throughout exploratory work on the scientific workflows research literature in order to apply the most relevant produced results to the evaluation of the Pingo system.

This section does its little contribution to the evaluation methodology of scientific workflow systems.  It defines what is /textbf{ideal execution time of a history of workflows}.  It also discusses an algorithm on how to compute it.  

Consider a sequence $H=(W_1, ..., W_n)$ of n workflows that have been submitted to the system over time, and let $S$ be the storage capacity of the file system.  In sequence $H$ time will be a discrete magnitude with $n$ time steps, and we say that $t = i$ corresponds to the time when workflow $W_i$ is submitted to the system. The effective life of dataset $d$ in sequence $H$ is defined as a tuple of time steps $(t1, t2)$, with $t1$ being the time step corresponding to the first workflow that creates dataset $d$, and  $t2$ being the time step corresponding to the last workflow that makes mention of dataset $d$ in $H$.  See Figure \ref{fig:effective_life} for an example of the effective life diagram of an hypothetical history of workflows.  Time steps are represented by vertical blue lines, and datasets are represented by horizontal black lines.  Dotted ranges in the black lines (as in dataset $d3$ and $d15$ in the figure) mean that the datasets were not part of the workflows submitted at the corresponding time steps.

An ideal system would only keep datasets in storage exactly for the duration of their effective life.  Let $D_H$ be the set of all datasets of a history $H$.  In our analysis, let $s: D_H \to \mathtt(N)$ be a function where $s(d_i)$ represents the storage that dataset $d_i$ occupies on the filesystem.  Let also $c: D_H \to \mathtt(N)$ be a function where $c(d_i)$ represents the average time that it takes to compute dataset $d_i$ by its corresponding action. At any given time $t$ there will be a set $M_t$ of datasets in storage, occupying an space $S' = \sum_{d \in M_t}{s(d)}$.  $S'$ must be less than $S$, otherwise, it is needed to remove some of the datasets present at that time to satisfy space constraint $S$.

Let $b(d)$ and $e(d)$ be the start time and end time, respectively of the effective life time of dataset $d$.  Since every dataset needs to exist for at least one time step (the time step corresponding to the workflow that created the dataset), it is only possible to consider removing datasets from $N_t \subseteq M_t$, where $d \in N_t$ if $b(d) < t$.  If the storage occupied by datasets in $N_t$ is less than $S' - S$, this will lead to a system failure that can only be effectively prevented by increasing $S$ or making the submitted workflow smaller.  For simplicity, this analysis ignores such situation.

\begin{figure}
\centering
\includegraphics[scale = 0.5]{effective_life_diagram}
\caption{Effective life diagram of datasets of hypothetical history of workflows.}
\label{fig:effective_life}
\end{figure}

Determining which datasets to keep and which ones to delete so as to optimize the total execution time of the history of workflows is an NP-Hard problem.  For an example reference, see the analysis of the problem of optimizing workflow computations with constrained storage for the case of two workflows as presented by \cite{bazzi}.  Because of that, we relax the constraints a little bit and ignore dependencies among datasets, so that what we call an \textbf{ideal solution} will not be an \textbf{optimal solution}.  Algorithm \ref{alg:ideal_computation_time} describes how to compute the ideal computational time of a sequence of workflows.  Note that it uses Algorithm \ref{alg:computation_time_left} as a subroutine.

\begin{algorithm}
\begin{singlespace}
\caption{Ideal Computation Time algorithm}
\label{alg:ideal_computation_time}
\begin{algorithmic}[1]
\Procedure{IdealComputationTime}{$S, H=(W_1, ..., W_n), b, e, c$}
	\State $totalTime \gets 0$
	\For{t from 1 to n}
		\State Let $M_t = \{ d\, |\, b(d) \leq t \wedge e(d) \geq t \}$
		\State Let $N_t = \{ d\, |\, b(d) < t \wedge e(d) \geq t \}$ \Comment{$N_t \subseteq M_t$}
		\State Let $P_t = \{ d\, |\, d \in W_t \}$ \Comment{$d \in M_t \centernot\implies d \in W_t$}
	\EndFor
	\State Let $M_H = (M_1, ..., M_n)$
	\State Let $N_H = (N_1, ..., N_n)$
	\State Let $P_H = (P_1, ,,,, P_n)$
	\For{t from 1 to n}
		\State Find subset $A$ of $N_t$ such that $\sum_{d \in A}{s(d)} \leq S$ and $\sum_{d \in A}{computationTimeLeft(d, P_H, t, n)}$ is maximum among all possible subsets of $N_t$.  (This is the classic Knapsack problem which has pseudopolynomial solutions and good approximations).
		\State Let $et$ be the time that workflow $W_t$ will take to compute its actions, assuming that $A$ is the set of datasets currently present in storage.
		\State $totalTime \gets totalTime + et$
	\EndFor
	\State \textbf{return} $totalTime$
\EndProcedure
\end{algorithmic}
\end{singlespace}
\end{algorithm}

\begin{algorithm}
\begin{singlespace}
\caption{Computation Time Left Subroutine}
\label{alg:computation_time_left}
\begin{algorithmic}[1]
\Procedure{ComputationTimeLeft}{$d, P_H, m, n, c$}
	\State $timeLeft \gets 0$
	\For{t from m to n}
		\If{$d \in P_t$}
			\State $timeLeft \gets timeLeft + c(d)$
		\EndIf
	\EndFor
	\State \textbf{return} $timeLeft$
\EndProcedure
\end{algorithmic}
\end{singlespace}
\end{algorithm}

There are to ways in which Algorithm \ref{alg:ideal_computation_time} does not find a global optimal value:
\begin{enumerate}
\item It does not take into account dependencies among datasets (as defined by the DAGs that represent the workflows).
\item At each time step $t$, it greedily finds a \textbf{local} minimum time that is added to the total result.
\end{enumerate}

But most good heuristic algorithms that can be proposed to find an ideal computation time of a sequence of workflows will be valid for our purposes, for as long as they provide results that are always lower than the real computation times taken by Pingo in computing those sequences of workflows.  As further research improves the \textbf{Decision System} of Pingo so that its prediction capabilities begin to improve, further research will be needed in order to close the gap between the concepts of the \textbf{ideal} and the \textbf{optimal} computation time.

\section{Evaluation Experiments}
In evaluation experiments, the main focus becomes evaluating the simple and adaptive algorithm families under different parameters.  All the experiments follow the same process:  (1) Generate a history of workflows using the generator that we have designed.  (2) Submit that history of workflows to the system, one workflow at a time, and see how much computation time does the system take using different algorithms. (3) When submitting the workflows to the system, always wait for the previous submitted workflow to compute in order to submit the next workflow. The purpose of that practice is to measure the gains in decrease of computation time that the algorithms provide under the most ideal circumstance. (4) Estimate the computation time it would have taken the system if we had used no algorithm.

*********BEGIN HERE ************
Each experiment was run 5 times using the same configuration parameters and the results were averaged.  The first experiment explores the effect of changing the storage constraint in the total computation time of a history of workflows.  It should be expected that as the the storage in the system increases, the computation time reduces.  In the second experiment the storage constrain remains fixed.  The second experiment explores instead how the different algorithms behave under different kinds of workflows, focusing on changing the percentage of actions from previous workflows that are included in new workflows.

The experiments run in the Hadoop distribution service provided by Amazon Cloud Computing, with the default settings, with the only difference that we also include Apache Oozie among the packages to install (it is not included by default).  Because of the limitations in time to run the experiments, and in storage (more storage costs more money), the computations of the actions that we run take only a few seconds, and their output is also relatively small.  The main interest running the experiments is to compare how different algorithms will compare against each other under different types of loads.  I ran a couple of experiments with more real-life computation time and output sizes and confirmed that the results stay consistent.

\subsection{How computation time is affected by the amount of storage in the system}
This experiment reports how the computation time is affected by the amount of storage space available in the system. The complete report of the parameters used to configure Pingo and the workflows' generator is in Appendix \ref{app:experiment_1_parameters}.  The number of actions for the experiment is 300, with an average of 10 actions per workflow, and each workflow repeating \textbf{half} the actions from previous workflows.  This means that on average, a workflow produced by the generator uses 5 new actions from the 300 possible actions it can choose, making the length of the generated sequences of workflows to be around 60.

Since each action will take an average of 10 seconds to finish, and its output will have an average size of 10MB, the total average time of our workflows should be 3000 seconds, and the average storage needed to save all of the actions is 3000MB.  In the experiment, the available space of the system is modified at increments of 500MB, starting at 500MB. 

Figure, \textbf{Computation time percentage} reports the computation time of the runs as a percentage of the computation time of the runs if all computations are performed. There are many interesting comments and conclusions that are obtained from those results. First of all, as expected, the computation time of the history of workflows decreases as the available storage in the system increases.  The effect can be more easily seen in the simple algorithm.  Another noticeable result is that as the storage capacity of the system increases, the simple algorithm approaches the performance of the adaptive algorithm.

Another important conclusion that is derived from the results reported in Figure \ref{fig:experiment1} is that the adaptive algorithm is fairly robust. Its performance is not affected as much by the available storage in the system as it is the case with the simple algorithm. It is remarkable to see how it achieves excellent performance even with storage capacity at 500MB.  To place the result in perspective, in the runs with storage capacity at 500MB, the adaptive algorithm performed 6 percent worse than in the runs with storage capacity at 2000MB, where it achieved its best results.


\begin{figure}
\centering
\includegraphics[scale = 1.1]{experiment1}
\caption{Computation time as storage increases}
\label{fig:experiment1}
\end{figure}

\subsection{How computation time is affected by the types of workflows in the history of workflows}
It is possible to produce many different kinds of workflows by playing with the parameters of the workflow generator.  This experiment only focuses in the parameter that determines the percentage of actions from previous workflows submitted to the system that are used by new workflows submitted to the system.  In the experiment the parameter changes at 10 percent increments, starting at 5 percent, and ending at 55 percent.  The amount of available space of the system is fixed at 500MB.   The complete report of the parameters used to configure Pingo and the workflows' generator in Appendix \ref{app:experiment_2_parameters}.

There are no surprises in the results of Figure \ref{fig:experiment2}.  Both, the simple and adaptive algorithms decrease their computation time as the percentage of previous actions that a workflow includes increases.  This result was expected since, the more previously computed actions a workflow includes, the more opportunities the Pingo system has to skip those computations since is likely that the outputs of those computations are available in storage.

\begin{figure}
\centering
\includegraphics[scale = 1.1]{experiment2}
\caption{Computation time percentage as percentage of actions from previous workflows increases}
\label{fig:experiment2}
\end{figure}

\section{Conclusions of our evaluations}
From the evaluations of the algorithms it can be concluded that the adaptive family of algorithms will perform better in the most basic scenarios than the simple algorithms by themselves.  In both algorithms, the definition used to assign value to each action was very simple, and it did not take into account the computation time of an action, only its frequency of usage.  Because of that, there is still room for improvements on the already promising results. 

For future evaluations, I propose the creation of more complex adaptive algorithms.  The current algorithm works well when the \textbf{look back} parameter does not probabilistically change much.  In real life scenarios, more complex adaptive algorithms should be able to detect the rate at which the look back parameter might be changing, and adapt to it.
The experiments designed were able to give some good insights into how the different algorithms we tested performed under different conditions. Because of that, I think that those evaluation experiments should serve as the foundation of any future evaluation methodology on the system. 