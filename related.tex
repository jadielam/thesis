\chapter{RELATED WORK}
\label{chap:related}
\section{Scientific workflows}
A workflow is the automation of a process, during which data is processed by a sequence of computations in a preordered way \citep{liu2015survey}. From a conceptual point of view, workflows can be divided into two types: business workflows and scientific workflows \citep{hollingsworth1995workflow, taylor2014workflows}. Scientific workflows are typically used for modeling and running scientific experiments. \cite{taylor2014workflows} defines scientific workflows as the assembly of complex sets of scientific data processing activities with data dependencies between them. More simple workflows can be represented as sequences (pipelines) of activities, but the most general representation is DAG, where nodes correspond to data processing actions and edges represent the data dependencies.  Scientific workflows must be fully reproducible \citep{barker2007scientific}. That is, the same results must be obtained after repeated executions of the computations of the workflow if the same data was used as input. Such requirement introduces the opportunity to store intermediate datasets to optimize the execution of computations of workflows.

In a workflow, an activity describes a piece of work that forms a logical step within the workflow representation.  The associated data in an activity consists of the input data and configuration parameters.  The execution of an activity is a job or task. In the literature, the terms \textbf{activity}, \textbf{job} and \textbf{task} are sometimes used interchangeably and special attention needs to be given to the context where the term is used to determine if they refer to the definition or to the execution of a task.
 
\section{Scientific Workflow Management Systems}
A Workflow Management System is a system that defines, creates and manages the execution of workflows. Many seminal works on the topic of workflow management systems began to appear in the mid 2000's \citep[e.g.]{yu2005taxonomy, fox2006special, gil2007examining}, and many workflow systems were developed, such as Kepler \citep{altintas2004kepler}, Taverna \citep{oinn2006taverna} and the e-Science project \citep{deelman2009workflows}. These legacy systems provided the foundations for the field of scientific workflows, but were designed with the intent of executing computations in local standalone machines.

A more advanced example is Pegasus \citep{singh2008workflow}, a workflow management system for scientific applications.  It enables workflows to be executed both locally and on a cluster of computers in a simultaneous manner.  It has a rich set of APIs that allow the construction and representation of workflows as DAGs.  It also has more advanced job scheduling and monitoring facilities than previous systems.  

One important capability that has been added as a functionality to some of these systems (see \cite{yuan2012data}) is the ability to store the computations of intermediate datasets with the purpose of optimizing the computation time of future workflows that are to be computed by the system and that make use of those intermediate datasets. For a scientific workflow system, there are two costs associated with running workflows.  On the one hand there is execution cost associated for running the workflow and on the other hand there is the storage cost for storing intermediate datasets.  Finding the optimal balance is at the heart of this problem.  This optimization problem is ofter formulated under the realistic assumption that there is an upper bound available for storage and trying to make the best use of the available storage.  The problem is also formulated as a cost optimization problem where there are no a priori bounds on storage, and costs are associated with units of computation and storage.  In this thesis I adopt the bounded-storage formulation.

Some systems research has included in their efforts research on the topic of optimizations for different versions of the data reuse problem.  \cite{ramakrishnan2007scheduling} addressed the problem of minimizing the amount of space a workflow requires by removing datasets at run-time when they are no longer required.  As mentioned previously, the research of \cite{yuan2012data} proposes a strategy to find a trade-off between computation cost and storage cost. Also, some theoretical research has been done on the data reuse problem itself. For example, \cite{adams2009maximizing} proposes a model to represent the trade-off of computation cost and storage cost, but does not give any strategy to solve the optimization problem that can be derived from their model. \cite{yuan2011demand} presented two algorithms as the minimum cost benchmark of the data reuse problem, one for the case of linear-structure workflows, which takes $O(n^4)$ time, and a general algorithm for parallel structure. \cite{cheng2015new} improve on the work of \cite{yuan2011demand} by presenting a new algorithm for the "linear-structure" type of workflows that runs in $O(n^3)$ time. \cite{zohrevandi2013bounded} formally introduce the data reuse problem under the assumption that the workflows to optimize are known before-hand. They model the problem using a non-linear integer programming formulation and show that  it is NP-Hard.  They also propose two algorithms: a branch and bound optimal algorithm, and a heuristic algorithm that is on average within 1\% of the optimal answer.
\section{Parallel Processing Frameworks}

The scale of computations have been growing with time, and the ability of the systems cited above to process large amounts of data and to execute the placement of task execution on a distributed environment is limited or can only be done on High Performance Computing (HPC) systems of expensive hardware.  Orthogonal to the development of workflow management systems, distributed parallel processing frameworks have been designed and developed to meet the growing demands of computation.  

One of such frameworks is MapReduce \citep{dean2008mapreduce}. It was originally developed by Google as a proprietary product to process large amounts of unstructured or semi-structured data clusters of machines with commodity hardware. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. There are multiple implementations of the framework, the most commonly used being part of the Hadoop ecosystem \citep{white2012hadoop}.

\cite{islam2012oozie} introduced Apache Oozie, which is Apache Hadoop's workflow and scheduling system.  Its workflow definition API rivals that of Pegasus, while it takes advantage of the superior scalability of the Hadoop ecosystem. Unfortunately, it does not provide any capability to optimize the computation of workflows by saving the output of intermediate datasets in the hope of skipping the computation of future actions submitted to the system.