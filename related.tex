\chapter{RELATED WORK}
\label{chap:related}
\section{Scientific workflows}
A workflow is the automation of a process, during which data is processed by a sequence of computations in a preordered way. \citep{liu2015survey} From a conceptual point of view, workflows can be divided into two types: business workflows and scientific workflows \citep{hollingsworth1995workflow, taylor2014workflows}. Scientific workflows are typically used for modeling and running scientific experiments. \cite{taylor2014workflows} defines scientific workflows as the assembly of complex sets of scientific data processing activities with data dependencies between them. More simple workflows can be represented as sequences (pipelines) of activities, but the most general representation is a directed acyclic graph (DAG), where nodes correspond to data processing actions and edges represent the data dependencies.  Scientific workflows must be fully reproducible \citep{barker2007scientific}. That is, the same results must be obtained after repeated executions of the computations of the workflow if the same data was used as input. Such requirement introduces the opportunity to store intermediate datasets to optimize the execution of computations of workflows.

In a workflow, an activity describes a piece of work that forms a logical step within the workflow representation.  The associated data in an activity consists of the input data and configurable parameters.  The execution of an activity is a job or task. In the workflow literature, these terms are sometimes used interchangeably, and special attention needs to be given to the context where the term is used.
 
\section{Scientific Workflow Management Systems}
A Workflow Management System is a system that defines, creates and manages the execution of workflows. Many seminal works on the topic of workflow managing systems began to appear in the mid 2000's \citep[e.g.]{yu2005taxonomy, fox2006special, gil2007examining}, and many workflow systems were developed, such as the e-Science project\citep{deelman2009workflows}, Kepler\citep{altintas2004kepler} and Taverna\citep{oinn2006taverna}. These legacy systems provided the foundations for the field of scientific workflows, but were designed with the intent of executing computations in local standalone machines.

A more recent example is Pegasus \citep{singh2008workflow}, a workflow management system for scientific applications.  It enables workflows to be executed both locally and on a cluster of computers in a simultaneous manner.  It has a rich set of APIs that allow the construction and representation of workflows as Directed Acyclic Graphs (DAGs).  It also has more advanced job scheduling and monitoring facilities than previous systems.  

One important capability that has been added as a functionality to some of these systems (see \cite{yuan2012data}) is the ability to store the computations of intermediate datasets with the purpose of optimizing the computation time of future workflows that are to be computed by the system and that make use of those intermediate datasets. For a scientific workflow system, storing all the intermediate data generated during workflow executions may cause high storage cost.  On the contrary, if we delete all the intermediate data and regenerate them every time when ever needed, the computation cost of the system may also be very high. Because of that, good tradeoffs need to be found to solve the problem.  They are sometimes achieved by constraining the amount of storage space available to the system, and other times by assigning cost to both storage space and computation time and designing decision algorithms that optimize for that cost. 

Some systems research has included in their efforts research on the topic of optimizations for different versions of the data reuse problem.  \cite{ramakrishnan2007scheduling} addressed the problem of minimizing the amount of space a workflow requires by removing datasets at run-time when they are no longer required.  As mentioned previously, the research of \cite{yuan2012data} proposes a strategy to find a trade-off between computation cost and storage cost. Also, some specific research has been done on the data reuse problem itself from a more purely theoretical perspective. For example, \cite{adams2009maximizing} proposes a model to represent the trade-off of computation cost and storage cost, but does not give any strategy to solve the optimization problem that can be derived from their model. \cite{zohrevandi2013bounded} formally introduces the data reuse problem under the assumption that the workflows to optimize are known before-hand. They model the problem using a non-linear integer programming formulation and show that  it is NP-Hard.  They also propose a branch and bound optimal algorithm for it.
\section{Parallel Processing Frameworks}

The scale of computations have been growing with time, and the ability of the systems cited above to process large amounts of data and to execute the placement of task execution on a distributed environment is limited or can only be done on High Performance Computing (HPC) systems of expensive hardware.  Orthogonal to the development of workflow management systems, distributed parallel processing frameworks have been designed and developed to meet the growing demands of computation.  

One of such frameworks is MapReduce \citep{dean2008mapreduce}. It was originally developed by Google as a proprietary product to process large amounts of unstructured or semi-structured data clusters of machines with commodity hardware. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. There are multiple implementations of the framework, the most commonly used being part of the Hadoop ecosystem \citep{white2012hadoop}.

\cite{islam2012oozie} introduced Apache Oozie, which is Apache Hadoop's workflow and scheduling system.  Its workflow definition API rivals that of Pegasus, while it takes advantage of the superior scalability of the Hadoop ecosystem. Unfortunately, it does not provide any capability to optimize the computation of workflows by saving the output of intermediate datasets in the hope of skipping the computation of future actions submitted to the system.